{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e769709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load .dockerignore\n",
      "#1 transferring context: 2B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load build definition from Dockerfile\n",
      "#2 transferring dockerfile: 657B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.10-slim-buster\n",
      "#3 DONE 1.3s\n",
      "\n",
      "#4 [1/5] FROM docker.io/library/python:3.10-slim-buster@sha256:37aa274c2d001f09b14828450d903c55f821c90f225fdfdd80c5180fcca77b3f\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [internal] load build context\n",
      "#5 transferring context: 38B done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [4/5] RUN pip install torch torchvision torchaudio\n",
      "#6 CACHED\n",
      "\n",
      "#7 [2/5] WORKDIR /app\n",
      "#7 CACHED\n",
      "\n",
      "#8 [3/5] RUN pip install transformers uvicorn fastapi\n",
      "#8 CACHED\n",
      "\n",
      "#9 [5/5] COPY ./inference_app.py /app/inference_app.py\n",
      "#9 CACHED\n",
      "\n",
      "#10 exporting to image\n",
      "#10 exporting layers done\n",
      "#10 writing image sha256:b0861f460371df98991569830f48838143aa761d804ed9729004928a58fe0cf6 done\n",
      "#10 naming to docker.io/library/huggingface-inference done\n",
      "#10 DONE 0.0s\n",
      "\n",
      "What's Next?\n",
      "  View a summary of image vulnerabilities and recommendations â†’ docker scout quickview\n"
     ]
    }
   ],
   "source": [
    " !docker build -t huggingface-inference ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad94d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd5f3996312cf86cef736bebe5d8a2be86e0a22ab43aa67bce92d24ffb8929ce\n"
     ]
    }
   ],
   "source": [
    "!docker run -d -p 8000:8000 huggingface-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce992d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[{'label': 'non-toxic', 'score': 0.9990329742431641}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the endpoint URL\n",
    "url = \"http://localhost:8000/predict\"\n",
    "\n",
    "# Define the data to send as a dictionary\n",
    "data = {\n",
    "    \"text\": \"time to end global warming\"\n",
    "}\n",
    "\n",
    "# Send a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response\n",
    "print(response.status_code)  # HTTP status code\n",
    "print(response.json())       # JSON response content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e6d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
